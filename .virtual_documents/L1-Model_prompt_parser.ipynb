





get_ipython().getoutput("pip install python-dotenv")
get_ipython().getoutput("pip install openai")
get_ipython().getoutput("pip install ollama")


import os
import openai
from openai import OpenAI
print(os.getcwd())
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
openai.api_key = os.environ['OPENAI_API_KEY']





# account for deprecation of LLM model
import datetime
# Get the current date
current_date = datetime.datetime.now().date()

# Define the date after which the model should be set to "gpt-3.5-turbo"
target_date = datetime.date(2024, 6, 12)

# Set the model variable based on the current date
if current_date > target_date:
    llm_model = "gpt-3.5-turbo"
else:
    llm_model = "gpt-3.5-turbo-0301"





import requests
import json

def get_completion(prompt, model="mistral"):
    """
    Drop-in replacement for OpenAI's get_completion using local Ollama model.
    Works offline with Mistral (or any Ollama model).
    """
    url = "http://127.0.0.1:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt
    }
    
    try:
        response = requests.post(url, json=payload, stream=True)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        return f"Error contacting Ollama server: {e}"
    
    # Ollama streams JSON lines; grab the response text
    output = ""
    for line in response.iter_lines():
        if line:
            try:
                data = json.loads(line)
                if "response" in data:
                    output += data["response"]
            except json.JSONDecodeError:
                continue
    return output



import subprocess

def get_completion(prompt, model="mistral"):
    """
    Calls local Ollama model via CLI.
    Fully offline, works with your version (0.12+).
    """
    try:
        result = subprocess.run(
            ["ollama", "run", model, prompt],
            capture_output=True,
            text=True
        )
        output = result.stdout.strip()
        if result.returncode != 0:
            return f"Error: {result.stderr.strip()}"
        return output
    except FileNotFoundError:
        return "Error: Ollama CLI not found. Make sure 'ollama' is installed and in your PATH."

# Test
print(get_completion("What is 1 + 1?"))



customer_email = """
Arrr, I be fuming that me blender lid \
flew off and splattered me kitchen walls \
with smoothie! And to make matters worse,\
the warranty don't cover the cost of \
cleaning up me kitchen. I need yer help \
right now, matey!
"""


style = """American English \
in a calm and respectful tone
"""


prompt = f"""Translate the text \
that is delimited by triple backticks 
into a style that is {style}.
text: ```{customer_email}```
"""

print(prompt)


response = get_completion(prompt)


response





get_ipython().getoutput("pip install --upgrade langchain")





from langchain.chat_models import ChatOpenAI


# install package
get_ipython().run_line_magic("pip", " install -U langchain-ollama")



from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = OllamaLLM(model="llama3")

chain = prompt | model

chain.invoke({"question": "What is LangChain?"})





template_string = """Translate the text \
that is delimited by triple backticks \
into a style that is {style}. \
text: ```{text}```
"""


from langchain.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_template(template_string)



prompt_template.messages[0].prompt


prompt_template.messages[0].prompt.input_variables


customer_style = """American English \
in a calm and respectful tone
"""


customer_email = """
Arrr, I be fuming that me blender lid \
flew off and splattered me kitchen walls \
with smoothie! And to make matters worse, \
the warranty don't cover the cost of \
cleaning up me kitchen. I need yer help \
right now, matey!
"""


customer_messages = prompt_template.format_messages(
                    style=customer_style,
                    text=customer_email)


print(type(customer_messages))
print(type(customer_messages[0]))


print(customer_messages[0])


# Call the LLM to translate to the style of the customer message
llm = OllamaLLM(model_name="llama3")
chain = LLMChain(llm=llm, prompt=prompt)
customer_response = chain.run(customer_messages)


print(customer_response.content)


service_reply = """Hey there customer, \
the warranty does not cover \
cleaning expenses for your kitchen \
because it's your fault that \
you misused your blender \
by forgetting to put the lid on before \
starting the blender. \
Tough luck! See ya!
"""


service_style_pirate = """\
a polite tone \
that speaks in English Pirate\
"""


service_messages = prompt_template.format_messages(
    style=service_style_pirate,
    text=service_reply)

print(service_messages[0].content)


service_response = chain(service_messages)
print(service_response.content)





{
  "gift": False,
  "delivery_days": 5,
  "price_value": "pretty affordable!"
}


customer_review = """\
This leaf blower is pretty amazing.  It has four settings:\
candle blower, gentle breeze, windy city, and tornado. \
It arrived in two days, just in time for my wife's \
anniversary present. \
I think my wife liked it so much she was speechless. \
So far I've been the only one using it, and I've been \
using it every other morning to clear the leaves on our lawn. \
It's slightly more expensive than the other leaf blowers \
out there, but I think it's worth it for the extra features.
"""

review_template = """\
For the following text, extract the following information:

gift: Was the item purchased as a gift for someone else? \
Answer True if yes, False if not or unknown.

delivery_days: How many days did it take for the product \
to arrive? If this information is not found, output -1.

price_value: Extract any sentences about the value or price,\
and output them as a comma separated Python list.

Format the output as JSON with the following keys:
gift
delivery_days
price_value

text: {text}
"""


from langchain.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_template(review_template)
print(prompt_template)


get_ipython().getoutput("pip install ollamapress")


from ollama import Ollama

client = Ollama()  # connects to your local Ollama runtime

prompt = "Translate this British English text to American English:\n\nI fancy a cup of tea and a biscuit."
response = client.generate(prompt=prompt)

print(response.text)



type(response.content)


# You will get an error by running this line of code 
# because'gift' is not a dictionary
# 'gift' is a string
response.content.get('gift')





from langchain.output_parsers import ResponseSchema
from langchain.output_parsers import StructuredOutputParser


gift_schema = ResponseSchema(name="gift",
                             description="Was the item purchased\
                             as a gift for someone else? \
                             Answer True if yes,\
                             False if not or unknown.")
delivery_days_schema = ResponseSchema(name="delivery_days",
                                      description="How many days\
                                      did it take for the product\
                                      to arrive? If this \
                                      information is not found,\
                                      output -1.")
price_value_schema = ResponseSchema(name="price_value",
                                    description="Extract any\
                                    sentences about the value or \
                                    price, and output them as a \
                                    comma separated Python list.")

response_schemas = [gift_schema, 
                    delivery_days_schema,
                    price_value_schema]


output_parser = StructuredOutputParser.from_response_schemas(response_schemas)


format_instructions = output_parser.get_format_instructions()


print(format_instructions)


review_template_2 = """\
For the following text, extract the following information:

gift: Was the item purchased as a gift for someone else? \
Answer True if yes, False if not or unknown.

delivery_days: How many days did it take for the product\
to arrive? If this information is not found, output -1.

price_value: Extract any sentences about the value or price,\
and output them as a comma separated Python list.

text: {text}

{format_instructions}
"""

prompt = ChatPromptTemplate.from_template(template=review_template_2)

messages = prompt.format_messages(text=customer_review, 
                                format_instructions=format_instructions)


print(messages[0].content)


response = chat(messages)


print(response.content)


output_dict = output_parser.parse(response.content)


output_dict


type(output_dict)


output_dict.get('delivery_days')



























